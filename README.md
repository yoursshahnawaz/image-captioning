# Image Captioning using Deep Learning

_A Deep Learning based model to generate relevant caption for an Image in Flickr 8K dataset. It uses CNN as an Image Encoder to extract features from an image and RNN as an Image Decoder to produce the corresponding caption for the image._

## Implementations and Details

### Data Collection

Flickr8K dataset has been downloaded from the internet. It consists of more than 8000 images of which 6000 images has been used for training model and the remaining will be used for model testing.
Image dataset:

![dataset](https://user-images.githubusercontent.com/47656810/122646899-293de880-d13f-11eb-8ba6-18e9b1eb96a1.jpg)

Text dataset:

![textdataset](https://user-images.githubusercontent.com/47656810/122647055-f8aa7e80-d13f-11eb-8cbd-9eac6ce86f13.jpg)

### Data Preprocessing – Text:

All letters in the captions are converted to lowercase, and all non-alphabetic characters are omitted. Since every image has five captions in the dataset, we have transformed it into a list consisting of all the five captions with each caption starting with the keyword ‘startseq’ and ending with the keyword ‘endseq’ in order to denote the starting and ending of a sentence.

![processeddesc](https://user-images.githubusercontent.com/47656810/122647146-5b037f00-d140-11eb-9df4-50a7b8326c46.jpg)

### Data Preprocessing - Images Data Collection

Images are nothing but input (X) to our model. As you may already know that any input to a model must be given in the form of a vector. We need to convert every image into a fixed sized vector which can then be fed as input to the neural network.
This model was trained on the VGG Model to perform image classification on 6000 different images. However, our purpose here is not to classify the image but just get a fixed-length informative vector for each image. This process is called automatic feature engineering.

![extractedfeatures](https://user-images.githubusercontent.com/47656810/122647162-6bb3f500-d140-11eb-9f3c-70e2d1b58046.jpg)

### Training Data Collection

As already mentioned, we are using Flickr8K dataset for the training and testing of the model. Flickr8K is an open-source dataset which if freely available. There are 8000 images in this dataset, each with five captions.

## RESULTS

The model is being evaluated using the captions created for the test dataset. For a picture in the test dataset, captions are generated and compared to the original captions for the image presented with the dataset.

![original](https://user-images.githubusercontent.com/47656810/122647168-72426c80-d140-11eb-923e-1b7a2a8834c3.png)

#### Caption generated by our model

![captions](https://user-images.githubusercontent.com/47656810/122647180-8be3b400-d140-11eb-9299-797d0be6771f.jpg)

## Installation

- Hardware Requirements:

  - Windows Operating System
  - Anaconda

- Software Requirements:
  - Operating System: Windows 10
  - Language: Python
  - Database: Flickr 8K database.
  - Libraries: Keras and Tensorflow for Machine learning models.
- Dependencies:
  - Keras 2.0.7
  - Numpy
  - Pandas 0.20.3
  - Matplotlib
  - Pickle

#### Installation procedure

User receives the description of Image input by them. The user can get description in the
form of text.

1. Open the model.
2. Run the Program.
3. Input an image.
4. Image caption is generated
